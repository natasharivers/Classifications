{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan --> Acquire --> Prepare --> Explore --> Model --> Deliver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we evaluate?\n",
    "\n",
    "- we have to quanitify model's performance\n",
    "- can compare to other models (decision tree vs k nearest neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab:\n",
    "\n",
    "- **Classifier**\n",
    "    - Binary\n",
    "    - Muli-Class\n",
    "    \n",
    "- **Evaluation Metric**: how we put a number on classifier's performance\n",
    "\n",
    "- **Label/Target/outcome**: the thing we are trying to predict\n",
    "\n",
    "- **Actual and Predicted Values**:\n",
    "    - Actual: ground truth from dataset, known ahead of time\n",
    "    - Predicted: ML model is predicting\n",
    "    - Compare actual and predicted to see how close they match\n",
    "    \n",
    "- **Classification Outcomes**:\n",
    "    - True Postitive(TP): predict positive and it is positive\n",
    "    - True Negative(TN): predict negative and its negative\n",
    "    - False Positive (FP): predict pos but its negative\n",
    "    - False Negative (FN): predict neg but its positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO CALCULATE HOW GOOD OUR CLASSIFIER IS\n",
    "\n",
    "### Focus on:\n",
    "\n",
    "- **Accuracy**: how well the model does overall ( number of times you get it right!)\n",
    "    - (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "- **Recall**: how many of the actually positive cases did we catch\n",
    "     - TP/ (TP + FN)\n",
    "     - sensitivity: aka recall\n",
    "     - specificity: recall for negative class\n",
    "    \n",
    "- **Precision**: times we predicted the positive cases\n",
    "     - TP/ (TP+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example:\n",
    "Predict the Weather\n",
    "- rain (+)\n",
    "- no rain (-)\n",
    "\n",
    "- **TP**: bring an umbrella for rain, it rains\n",
    "- **TN**: no umbrella, no rain\n",
    "- **FP**: bring an umbrella, no rain\n",
    "- **FN**: no umbrella, it rains\n",
    "\n",
    "- **Taking action**: associated with positive value\n",
    "        - (ex) bring an umbrella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 day weather prediction model\n",
    "- Actual:       +, +, -, +, -, -, +\n",
    "- Predicated: -, +, +, +, +, -, +\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "- Accuracy: 4/7 correct, 57% accurate\n",
    "- Precision: 3/5, 60% precise\n",
    "- Recall: 3/4, 75% recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for this model. Recall matters most. You don't want to get wet but you don't care if you bring an umbrella and it doesn't rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:\n",
    "### In a model that predicts (+) EVERY time\n",
    "\n",
    "- Actual:       +, +, -, +, -, -, +\n",
    "- Predicated: +, +, +, +, +, +, +\n",
    "    \n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "- Accuracy: 4/7 correct, 57% accurate\n",
    "- Precision: 4/5, 57% precise\n",
    "- Recall: 4/4, 100% recall    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:\n",
    "### In a model that predicts (-) EVERY time\n",
    "\n",
    "- Actual:       +, +, -, +, -, -, +\n",
    "- Predicated: -, -, -, -, -, -, -\n",
    "    \n",
    "\n",
    "<br>\n",
    "\n",
    "- Accuracy: 3/7 correct, 43% accurate\n",
    "- Precision: 0/0, undefined\n",
    "- Recall: 0/4, 0% recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Senario:\n",
    "- you are bringing coffee to a meeting\n",
    "- need to predict if each person at the meeting wants coffee or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lola: really good coffee, but super expensive\n",
    "    - cost of a FP is higher than FN\n",
    "    - precision is better here because buying a cup of coffee for someone who won't drink it is expensive\n",
    "    - We want to be sure about our positive predictions\n",
    "    ~~~~\n",
    "    \n",
    "- taco cabana: bad coffee, but cheap\n",
    "    - cost of a FN is higher than FP\n",
    "    - recall because the coffee is cheap, its not bad to buy a cheap coffee for someone who won't drink it; worse to not get someone coffee who wanted it\n",
    "    ~~~~\n",
    "    \n",
    "- meeting with super important client\n",
    "    - cost of FN is higher, because they might be offended if we dont' get them coffee\n",
    "    - cost of FN == not signing a contract\n",
    "    - recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk through the steps:\n",
    "\n",
    "- **Step 1**. list out posible outcomes\n",
    "- **Step 2**. spec out what do outcomes mean\n",
    "\n",
    "<br>\n",
    "\n",
    "- FP: Buy a coffee for someone who won't drink it\n",
    "- FN: Don't buy a coffee for someone who wanted one\n",
    "- TP: Buy a coffee for someone who will drink it\n",
    "- TN: Don't buy a coffee for someone who wouldn't drink it anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senario 2:  \n",
    "Build a classifier to predict whether a given face should unlock the iPhone.\n",
    "\n",
    "- What is the positive and negative case?\n",
    "    - positive: phone unlocking\n",
    "    - negative: phone staying locked\n",
    "    \n",
    "<br>\n",
    "\n",
    "- What are the possible outcomes?\n",
    "    - FP: Phone unlocks but it shouldn't\n",
    "    - FN: Phone doesn't unlock but it should\n",
    "    - TP: Phone unlocks and it should\n",
    "    - TN: Phone doesn't unlock and it shouldn't \n",
    "    \n",
    "<br>\n",
    "\n",
    "- What are the costs of the outcomes?\n",
    "    - FP is most costly. We do not want a phone to unlock unless it should \n",
    "    \n",
    "<br>\n",
    "\n",
    "- Which metric should we use?\n",
    "    - precision is the metric we should use. cost of false negative is not very high. We do not really care if it doesn't unlock and should\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: \n",
    "Predict whether an email is spam or not. Emails marked as spam skip the inbox and go to the spam folder.\n",
    "\n",
    "- What is the positive and negative case?\n",
    "    - positve: is marked as spam\n",
    "    - negative: not marked as span\n",
    "\n",
    "<br>\n",
    "\n",
    "- What are the possible outcomes?\n",
    "    - FP: not spam marked as spam\n",
    "    - FN: marked spam but is not spam\n",
    "    - TP: marked spam, is spam\n",
    "    - TN: marked not spam, is not spam\n",
    "    \n",
    "<br>\n",
    "\n",
    "- What are the costs of the outcomes?\n",
    "    - spam marked but it is not spam, missing email you dont want to miss\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Which metric should we use?\n",
    "    - accuracy: we don't want inbox emails to end up in spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 4: \n",
    "Predict whether an email is a phishing attempt. When we predict positive, show an additional banner warning the user that this might be a phishing email.\n",
    "\n",
    "- What is the positive and negative case?\n",
    "    - positve: \n",
    "    - negative: \n",
    "\n",
    "<br>\n",
    "\n",
    "- What are the possible outcomes?\n",
    "    - FP: \n",
    "    - FN: \n",
    "    - TP: \n",
    "    - TN: \n",
    "\n",
    "<br>\n",
    "\n",
    "- What are the costs of the outcomes?\n",
    "    - \n",
    "    \n",
    "<br>\n",
    "\n",
    "- Which metric should we use?\n",
    "    - precision: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual prediction\n",
       "0     coffee  no coffee\n",
       "1  no coffee  no coffee\n",
       "2  no coffee     coffee\n",
       "3     coffee     coffee\n",
       "4     coffee     coffee\n",
       "5     coffee     coffee\n",
       "6  no coffee  no coffee\n",
       "7     coffee  no coffee"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe with two columns (predicted and actual)\n",
    "df = pd.DataFrame({\n",
    "    'actual': ['coffee', 'no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'coffee'],\n",
    "    'prediction': ['no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'no coffee'],\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>coffee</th>\n",
       "      <th>no coffee</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no coffee</th>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual      coffee  no coffee\n",
       "prediction                   \n",
       "coffee          -3         -1\n",
       "no coffee       -2         -2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Confusion Matrix\n",
    "- pd.crosstab(df.prediction, df.actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TP: predicted coffee + actual is coffee\n",
    "- FP: predicted coffee, but they didn't like coffee\n",
    "- FN: predicted no coffee, but really they liked coffee\n",
    "- TN: predicted no coffee, actual no coffee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "- accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "- (3 + 2) / (3 + 1 + 2 +2) = 62.5%\n",
    "\n",
    "<br>\n",
    "\n",
    "- precision: TP / (TP + FP)\n",
    "- 3 / (3 + 1) = 75%\n",
    "- FP is more costly than FN\n",
    "\n",
    "<br>\n",
    "\n",
    "- recall: TP / (TP + FN)\n",
    "- 3 / (3 + 2) = 60%\n",
    "- FN is more costly than FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column\n",
    "df['baseline'] = 'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual prediction baseline\n",
       "0     coffee  no coffee   coffee\n",
       "1  no coffee  no coffee   coffee\n",
       "2  no coffee     coffee   coffee\n",
       "3     coffee     coffee   coffee\n",
       "4     coffee     coffee   coffee\n",
       "5     coffee     coffee   coffee\n",
       "6  no coffee  no coffee   coffee\n",
       "7     coffee  no coffee   coffee"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model accuracy\n",
    "(df.actual == df.prediction).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline accuracy\n",
    "(df.actual == df.baseline).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual prediction baseline\n",
      "2  no coffee     coffee   coffee\n",
      "3     coffee     coffee   coffee\n",
      "4     coffee     coffee   coffee\n",
      "5     coffee     coffee   coffee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision -- how good are our positive predictions\n",
    "# precision -- model performance | pred +\n",
    "\n",
    "#create a subset of data where we predicted the positive case\n",
    "subset = df[df.prediction == 'coffee']\n",
    "print(subset)\n",
    "(subset.prediction == subset.actual).mean()\n",
    "\n",
    "#this shows that our precision is 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   actual prediction baseline\n",
      "0  coffee  no coffee   coffee\n",
      "3  coffee     coffee   coffee\n",
      "4  coffee     coffee   coffee\n",
      "5  coffee     coffee   coffee\n",
      "7  coffee  no coffee   coffee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall -- how often do we get the actual positive cases\n",
    "# recall -- model performance | actual +\n",
    "\n",
    "#create a subset of data where actual value is positve\n",
    "subset = df[df.actual == 'coffee']\n",
    "print(subset)\n",
    "(subset.prediction == subset.actual).mean()\n",
    "\n",
    "#this shows that recall is 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual prediction baseline\n",
      "0     coffee  no coffee   coffee\n",
      "1  no coffee  no coffee   coffee\n",
      "2  no coffee     coffee   coffee\n",
      "3     coffee     coffee   coffee\n",
      "4     coffee     coffee   coffee\n",
      "5     coffee     coffee   coffee\n",
      "6  no coffee  no coffee   coffee\n",
      "7     coffee  no coffee   coffee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision\n",
    "subset = df[df.baseline == 'coffee']\n",
    "print(subset)\n",
    "(subset.baseline == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   actual prediction baseline\n",
      "0  coffee  no coffee   coffee\n",
      "3  coffee     coffee   coffee\n",
      "4  coffee     coffee   coffee\n",
      "5  coffee     coffee   coffee\n",
      "7  coffee  no coffee   coffee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall\n",
    "subset = df[df.actual == 'coffee']\n",
    "print(subset)\n",
    "(subset.baseline == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "positive: coffee\n",
      "\n",
      "         | accuracy | recall | precision\n",
      "         | -------- | ------ | ---------         \n",
      "   model |    62.5% |  60.0% |     75.0%\n",
      "baseline |    62.5% | 100.0% |     62.5%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive = 'coffee'\n",
    "\n",
    "# accuracy -- overall hit rate\n",
    "model_accuracy = (df.prediction == df.actual).mean()\n",
    "baseline_accuracy = (df.baseline == df.actual).mean()\n",
    "\n",
    "# precision -- how good are our positive predictions?\n",
    "# precision -- model performance | predicted positive\n",
    "subset = df[df.prediction == positive]\n",
    "model_precision = (subset.prediction == subset.actual).mean()\n",
    "subset = df[df.baseline == positive]\n",
    "baseline_precision = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "# recall -- how good are we at detecting actual positives?\n",
    "# recall -- model performance | actual positive\n",
    "subset = df[df.actual == positive]\n",
    "model_recall = (subset.prediction == subset.actual).mean()\n",
    "baseline_recall = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "\n",
    "print(f'''\n",
    "positive: {positive}\n",
    "\n",
    "         | accuracy | recall | precision\n",
    "         | -------- | ------ | ---------         \n",
    "   model | {model_accuracy:8.1%} | {model_recall:6.1%} | {model_precision:9.1%}\n",
    "baseline | {baseline_accuracy:8.1%} | {baseline_recall:6.1%} | {baseline_precision:9.1%}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
