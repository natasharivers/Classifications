{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Exercises:\n",
    "\n",
    "1. What is your baseline prediction? What is your baseline accuracy? remember: your baseline prediction for a classification problem is predicting the most prevelant class in the training dataset (the mode). When you make those predictions, what is your accuracy? This is your baseline accuracy.\n",
    "\n",
    "2. Fit the decision tree classifier to your training sample and transform (i.e. make predictions on the training sample)\n",
    "\n",
    "3. Evaluate your in-sample results using the model score, confusion matrix, and classification report.\n",
    "\n",
    "4. Compute: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n",
    "\n",
    "5. Run through steps 2-4 using a different max_depth value.\n",
    "\n",
    "6. Which model performs better on your in-sample data?\n",
    "\n",
    "7. Which model performs best on your out-of-sample data, the validate set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = prepare.prep_titanic(acquire.get_titanic_data())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get value counts of survived (1) vs did not survive (0)\n",
    "train.survived.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1. What is your baseline prediction? What is your baseline accuracy? remember: your baseline prediction for a classification problem is predicting the most prevelant class in the training dataset (the mode). When you make those predictions, what is your accuracy? This is your baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the baseline\n",
    "train[\"baseline\"] = 0\n",
    "baseline_accuracy = (train.survived == train.baseline).mean()\n",
    "print(f'Baseline accuracy is: {baseline_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2. Fit the decision tree classifier to your training sample and transform (i.e. make predictions on the training sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify columns that you want to use\n",
    "\n",
    "#only using the following columns\n",
    "X_cols = ['pclass', 'fare', 'alone', 'Q', 'S']\n",
    "\n",
    "#only trying to see who survived or died\n",
    "y_col = 'survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split your data - train, validate, test\n",
    "\n",
    "X_train, y_train = train[X_cols], train[y_col]\n",
    "X_validate, y_validate = validate[X_cols], validate[y_col]\n",
    "X_test, y_test = test[X_cols], test[y_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify model 1 \n",
    "model1 = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model 1 using train data\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get scores for train and validate, not using test yet\n",
    "print(f'training score: {model1.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {model1.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 is the prediction\n",
    "train['model1'] = model1.predict(X_train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #3. Evaluate your in-sample results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 score\n",
    "# code == model1.score(X_train, y_train)\n",
    "print(f'model1 score: {model1.score(X_train, y_train):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(train.survived, train.model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(classification_report(train.survived, train.model1, zero_division =True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #4. Compute: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get percentage, normalize=true\n",
    "pd.crosstab(train.survived, train.model1, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "positive- died (0)\n",
    "<br>\n",
    "negative - survived (1)\n",
    "\n",
    "- **True positive** - 59.96%\n",
    "- **False Postive** - 11.27% (predict they died, but they lived)\n",
    "- **True Negative** - 25.96%\n",
    "- **False Negative** - 1.81% (predict they lived, but they died)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classification_report(train.survived, train.model1, zero_division =True, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision = TP / (TP+FP)\n",
    "precision= 298 / (298+61)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computer is using the (1- survived) as a positive\n",
    "precision_score(train.survived, train.model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall = TP/ (TP+FN)\n",
    "recall= 298/ (298+9)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computer is using the (1- survived) as a positive\n",
    "recall_score(train.survived, train.model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy = TP+TN/(TP +TN+FN+FP)\n",
    "accuracy= (298+129) / (298+129+61+9)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(train.survived, train.model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR make classification_report its own value\n",
    "cr= pd.DataFrame(classification_report(train.survived, train.model1, zero_division =True, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call classification variable- column 1\n",
    "cr['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #5. Run through steps 2-4 using a different max_depth value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = DecisionTreeClassifier(max_depth=1)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(24,12))\n",
    "\n",
    "plot_tree(model2, feature_names=X_train.columns.tolist(), class_names=['died', 'survived'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_depth = 1 score\n",
    "# code == model2.score(X_train, y_train)\n",
    "print(f'model2 score: {model2.score(X_train, y_train):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2 gives a score of 63.18% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = DecisionTreeClassifier(max_depth=3)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(24,12))\n",
    "\n",
    "plot_tree(model3, feature_names=X_train.columns.tolist(), class_names=['died', 'survived'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_depth = 3 score\n",
    "# code == model3.score(X_train, y_train)\n",
    "print(f'model3 score: {model3.score(X_train, y_train):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3 gives a score of 69.01% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #6. Which model performs better on your in-sample data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 (with default max_depth=0) did the best of all three models with an accuracy of 85.92%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #7 Which model performs best on your out-of-sample data, the validate set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'model1 score: {model1.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'model2 score: {model2.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'model3 score: {model3.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 has the highest accuracy in validate set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid blue\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10.\n",
    "<br>\n",
    "\n",
    "2. Evaluate your results using the model score, confusion matrix, and classification report.\n",
    "<br>\n",
    "\n",
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n",
    "<br>\n",
    "\n",
    "4. Run through steps increasing your min_samples_leaf and decreasing your max_depth.\n",
    "<br>\n",
    "\n",
    "5. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?\n",
    "\n",
    "After making a few models, which one has the best performance (or closest metrics) on both train and validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import acquire\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = prepare.prep_titanic(acquire.get_titanic_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get to know data\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only using the following columns\n",
    "X_cols = ['pclass', 'fare', 'alone', 'Q', 'S']\n",
    "\n",
    "#only trying to see who survived or died\n",
    "y_col = 'survived'\n",
    "\n",
    "X_train, y_train = train[X_cols], train[y_col]\n",
    "X_validate, y_validate = validate[X_cols], validate[y_col]\n",
    "X_test, y_test = test[X_cols], test[y_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look to see if we have nulls or columns to drop\n",
    "train.info()\n",
    "\n",
    "#data looks to be ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at X_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #1 Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make our thing\n",
    "clf= RandomForestClassifier(min_samples_leaf = 1, max_depth = 10, random_state= 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the thing (ONLY on train set!!)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the thing (on training set)\n",
    "y_pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows an array of y_predictions\n",
    "#y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #2 Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model score\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model score for validate set\n",
    "clf.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['model5'] = clf.predict(X_train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix- created manually with crosstab\n",
    "pd.crosstab(train.survived, train.model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get percentage to answer question\n",
    "pd.crosstab(train.survived, train.model5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "pd.DataFrame(classification_report(y_train, y_pred, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #3 Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random Forest Model 5 Accuracy score is: {clf.score(X_train, y_train):.2%}')\n",
    "print(f'Random Forest Model 5 Precision score is: {precision_score(train.survived, train.model5):.2%}')\n",
    "print(f'Random Forest Model 5 Recall score is: {recall_score(train.survived, train.model5):.2%}')\n",
    "print(f'Random Forest Model 5 F1 score is: {f1_score(train.survived, train.model5):.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR \n",
    "rf= pd.DataFrame(classification_report(y_train, y_pred, output_dict=True))\n",
    "rf['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- positive- died (1) \n",
    "- negative - survived (0)\n",
    "<br>\n",
    "\n",
    "- True positive - 27.57%\n",
    "- False Postive - 10.66% (predict they lived, but they actually died)\n",
    "- True Negative - 57.95%\n",
    "- False Negative - 3.82% (predict they died, but they actually lived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #4 Run through steps increasing your min_samples_leaf and decreasing your max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model\n",
    "clf= RandomForestClassifier(min_samples_leaf = 3, max_depth = 15, random_state= 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model fit\n",
    "y_pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model train score\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model validate score\n",
    "clf.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model- insert column\n",
    "train['model6'] = clf.predict(X_train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model confusion matrix\n",
    "pd.crosstab(train.survived, train.model6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classification_report(y_train, y_pred, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random Forest Model 6 Accuracy score is: {clf.score(X_train, y_train):.2%}')\n",
    "print(f'Random Forest Model 6 Precision score is: {precision_score(train.survived, train.model6):.2%}')\n",
    "print(f'Random Forest Model 6 Recall score is: {recall_score(train.survived, train.model6):.2%}')\n",
    "print(f'Random Forest Model 6 F1 score is: {f1_score(train.survived, train.model6):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third RF model\n",
    "clf= RandomForestClassifier(min_samples_leaf = 2, max_depth = 20, random_state= 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third RF model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third RF model fit\n",
    "y_pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model train score\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model validate score\n",
    "clf.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third RF model- insert column\n",
    "train['model7'] = clf.predict(X_train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third RF model confusion matrix\n",
    "pd.crosstab(train.survived, train.model7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random Forest Model 6 Accuracy score is: {clf.score(X_train, y_train):.2%}')\n",
    "print(f'Random Forest Model 6 Precision score is: {precision_score(train.survived, train.model6):.2%}')\n",
    "print(f'Random Forest Model 6 Recall score is: {recall_score(train.survived, train.model6):.2%}')\n",
    "print(f'Random Forest Model 6 F1 score is: {f1_score(train.survived, train.model6):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third RF model\n",
    "clf= RandomForestClassifier(min_samples_leaf = 1, max_depth = 13, random_state= 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third RF model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third RF model fit\n",
    "y_pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model score train set\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second RF model score with validate set\n",
    "clf.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third RF model- insert column\n",
    "train['model7'] = clf.predict(X_train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third RF model confusion matrix\n",
    "pd.crosstab(train.survived, train.model7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random Forest Model 7 Accuracy score is: {clf.score(X_train, y_train):.2%}')\n",
    "print(f'Random Forest Model 7 Precision score is: {precision_score(train.survived, train.model7):.2%}')\n",
    "print(f'Random Forest Model 7 Recall score is: {recall_score(train.survived, train.model7):.2%}')\n",
    "print(f'Random Forest Model 7 F1 score is: {f1_score(train.survived, train.model7):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #5 What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Having run through multiple max_depth and min_sample_leaf:\n",
    "\n",
    "    - it appears that max_depth=13 is the optimal level, any max_depth above that gives SAME score.\n",
    "    - min_sample_leaf=1 (defaul) gives the optimal level, any min_sample_leaf above that, will give a lower score.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- clf= RandomForestClassifier(min_samples_leaf = 1, max_depth = 13, random_state= 123) <--- this gives highest score of **85.92%** accuracy in train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I would use Recall because we do not want to miss any positive cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid blue\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Exercise (May 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import model_fun_cindy\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "\n",
    "import acquire\n",
    "import prepare\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue working in your model file with the titanic dataset.\n",
    "\n",
    "1. Fit a K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)\n",
    "\n",
    "2. Evaluate your results using the model score, confusion matrix, and classification report.\n",
    "\n",
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n",
    "\n",
    "4. Run through steps 2-4 setting k to 10\n",
    "\n",
    "5. Run through setps 2-4 setting k to 20\n",
    "\n",
    "6. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?\n",
    "\n",
    "7. Which model performs best on our out-of-sample data from validate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1 Fit a K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.1250</td>\n",
       "      <td>First</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134.5000</td>\n",
       "      <td>First</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>39.6875</td>\n",
       "      <td>Third</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76.2917</td>\n",
       "      <td>First</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>29.916875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>146.5208</td>\n",
       "      <td>First</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex        age  sibsp  parch      fare  class  \\\n",
       "583         0       1    male  36.000000      0      0   40.1250  First   \n",
       "337         1       1  female  41.000000      0      0  134.5000  First   \n",
       "50          0       3    male   7.000000      4      1   39.6875  Third   \n",
       "218         1       1  female  32.000000      0      0   76.2917  First   \n",
       "31          1       1  female  29.916875      1      0  146.5208  First   \n",
       "\n",
       "     embark_town  alone  Q  S  \n",
       "583    Cherbourg      1  0  0  \n",
       "337    Cherbourg      1  0  0  \n",
       "50   Southampton      0  0  1  \n",
       "218    Cherbourg      1  0  0  \n",
       "31     Cherbourg      0  0  0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, validate, test = prepare.prep_titanic(acquire.get_titanic_data())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only using the following columns\n",
    "X_cols = ['pclass', 'fare', 'alone', 'Q', 'S']\n",
    "\n",
    "#only trying to see who survived or died\n",
    "y_col = 'survived'\n",
    "\n",
    "X_train, y_train = train[X_cols], train[y_col]\n",
    "X_validate, y_validate = validate[X_cols], validate[y_col]\n",
    "X_test, y_test = test[X_cols], test[y_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2 Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8430583501006036"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the thing\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "#fit the thing\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#score the thing\n",
    "knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:1\n",
      "training score: 84.31%\n",
      "validate score: 68.22%\n",
      "accuracy score: 60.67%\n",
      "________________________\n",
      "Model:2\n",
      "training score: 80.48%\n",
      "validate score: 68.22%\n",
      "accuracy score: 62.92%\n",
      "________________________\n",
      "Model:3\n",
      "training score: 79.88%\n",
      "validate score: 70.09%\n",
      "accuracy score: 57.87%\n",
      "________________________\n",
      "Model:4\n",
      "training score: 77.06%\n",
      "validate score: 69.16%\n",
      "accuracy score: 60.11%\n",
      "________________________\n",
      "Model:5\n",
      "training score: 77.46%\n",
      "validate score: 70.09%\n",
      "accuracy score: 62.36%\n",
      "________________________\n",
      "Model:6\n",
      "training score: 74.45%\n",
      "validate score: 66.82%\n",
      "accuracy score: 63.48%\n",
      "________________________\n",
      "Model:7\n",
      "training score: 75.05%\n",
      "validate score: 67.76%\n",
      "accuracy score: 64.61%\n",
      "________________________\n",
      "Model:8\n",
      "training score: 74.25%\n",
      "validate score: 68.22%\n",
      "accuracy score: 65.17%\n",
      "________________________\n",
      "Model:9\n",
      "training score: 74.85%\n",
      "validate score: 70.56%\n",
      "accuracy score: 66.85%\n",
      "________________________\n",
      "Model:10\n",
      "training score: 74.25%\n",
      "validate score: 71.50%\n",
      "accuracy score: 66.85%\n",
      "________________________\n",
      "Model:11\n",
      "training score: 74.45%\n",
      "validate score: 71.03%\n",
      "accuracy score: 66.85%\n",
      "________________________\n",
      "Model:12\n",
      "training score: 74.04%\n",
      "validate score: 71.03%\n",
      "accuracy score: 65.73%\n",
      "________________________\n",
      "Model:13\n",
      "training score: 73.84%\n",
      "validate score: 72.90%\n",
      "accuracy score: 67.42%\n",
      "________________________\n",
      "Model:14\n",
      "training score: 72.43%\n",
      "validate score: 70.56%\n",
      "accuracy score: 66.29%\n",
      "________________________\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 15):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(f'Model:{k}')\n",
    "    print(f'training score: {knn.score(X_train, y_train):.2%}')\n",
    "    print(f'validate score: {knn.score(X_validate, y_validate):.2%}')\n",
    "    print(f'accuracy score: {knn.score(X_test, y_test):.2%}')\n",
    "    print ('________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1: 60.67%\n",
      " 2: 62.92%\n",
      " 3: 57.87%\n",
      " 4: 60.11%\n",
      " 5: 62.36%\n",
      " 6: 63.48%\n",
      " 7: 64.61%\n",
      " 8: 65.17%\n",
      " 9: 66.85%\n",
      "10: 66.85%\n",
      "11: 66.85%\n",
      "12: 65.73%\n",
      "13: 67.42%\n",
      "14: 66.29%\n",
      "15: 66.85%\n",
      "16: 65.17%\n",
      "17: 66.29%\n",
      "18: 65.17%\n",
      "19: 65.73%\n"
     ]
    }
   ],
   "source": [
    "#this shows that K of 13 has best accuracy\n",
    "for k in range(1, 20):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    accuracy = knn.score(X_test, y_test)\n",
    "    print(f'{k:2d}: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #3 Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738430583501006"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the thing\n",
    "knn = KNeighborsClassifier(n_neighbors=13)\n",
    "\n",
    "#fit the thing\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#score the thing\n",
    "knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The accuracy for our model is 73.8431%\n",
      "    The True Positive Rate is 55.263%,    The False Positive Rate is 14.658%,\n",
      "    The True Negative Rate is 85.342%,    The False Negative Rate is 44.737%\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "\n",
      "    The positive is  'survived'\n",
      "    Confusion Matrix\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_dead</th>\n",
       "      <th>pred_survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_dead</th>\n",
       "      <td>True Negative: 262</td>\n",
       "      <td>False positive: 45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_survived</th>\n",
       "      <td>False Negative: 85</td>\n",
       "      <td>True Positive: 105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          pred_dead       pred_survived\n",
       "actual_dead      True Negative: 262  False positive: 45\n",
       "actual_survived  False Negative: 85  True Positive: 105"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "    Classification Report:\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dead</th>\n",
       "      <td>0.755043</td>\n",
       "      <td>0.853420</td>\n",
       "      <td>0.801223</td>\n",
       "      <td>307.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.738431</td>\n",
       "      <td>0.738431</td>\n",
       "      <td>0.738431</td>\n",
       "      <td>0.738431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.727522</td>\n",
       "      <td>0.703026</td>\n",
       "      <td>0.709435</td>\n",
       "      <td>497.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.734001</td>\n",
       "      <td>0.738431</td>\n",
       "      <td>0.731043</td>\n",
       "      <td>497.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "dead           0.755043  0.853420  0.801223  307.000000\n",
       "survived       0.700000  0.552632  0.617647  190.000000\n",
       "accuracy       0.738431  0.738431  0.738431    0.738431\n",
       "macro avg      0.727522  0.703026  0.709435  497.000000\n",
       "weighted avg   0.734001  0.738431  0.731043  497.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_fun_cindy.model_performs(X_train, y_train, knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #4 Run through steps 2-4 setting k to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7424547283702213"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the thing\n",
    "knn2 = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "#fit the thing\n",
    "knn2.fit(X_train, y_train)\n",
    "\n",
    "#score the thing\n",
    "knn2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #5 Run through steps 2-4 setting k to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6921529175050302"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the thing\n",
    "knn3 = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "#fit the thing\n",
    "knn3.fit(X_train, y_train)\n",
    "\n",
    "#score the thing\n",
    "knn3.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #6 What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVcElEQVR4nO3df5Afd33f8ecLSQ5nYnJOLZr6bNcmYytx67GFLzYpgZpQKhPaWLhNx1AKpZNxnGICndStnU4K05mOmFFKYBqDxzUmpCU4FFTZk6E+HBJs2hBGJ6RaFooSjQu2Tg6W04gfmmMsy+/+8f2e53ScpO/6bvX93t7zMaPR7X53v9/3d2+tl3c/u+9NVSFJ0qBeMuwCJEkri8EhSWrE4JAkNWJwSJIaMTgkSY2sHXYBy+m8886riy++eNhlSNKKsXPnzmeqan2TdToVHBdffDHT09PDLkOSVowk32y6jqeqJEmNGBySpEYMDklSIwaHJKkRg0OS1EinrqqS1I7tu2bYOrWfQ0dmOX98jNs2bWDzxolhl6UhMTgkndL2XTPcsW0Ps8eOAzBzZJY7tu0BMDxWKU9VSTqlrVP7XwiNObPHjrN1av+QKtKwGRySTunQkdlG89V9BoekUzp/fKzRfHWfwSHplG7btIGxdWtOmDe2bg23bdowpIo0bA6OSzqluQFwr6rSHIND0mlt3jhhUOgFnqqSJDVicEiSGjE4JEmNGBySpEYcHJdOYjn6M43Ke3SF22I0GBzSIpajP9OovEdXuC1Gh6eqpEUsR3+mUXmPrnBbjA6DQ1rEcvRnGpX36Aq3xegwOKRFLEd/plF5j65wW4wOg0NaxHL0ZxqV9+gKt8XocHBcWsRy9GcalffoCrfF6EhVDbuGZTM5OVnT09PDLkOSVowkO6tqssk6nqqSJDVicEiSGjE4JEmNODgu6YzoSruQrnyPpTA4JLWuK+1CuvI9lspTVZJa15V2IV35HktlcEhqXVfahXTleyyVwSGpdV1pF9KV77FUBoek1nWlXUhXvsdStRocSa5Psj/JgSS3n2SZ65LsTrI3ycPz5o8n+WySP02yL8lPt1mrpPZs3jjBlhuvYGJ8jAAT42NsufGKFTeg3JXvsVSttRxJsgb4M+CNwEFgB/DWqvr6vGXGgT8Grq+qJ5K8oqqe7r/2SeDLVXVPkrOAs6vqyKk+05YjktTMqLUcuQY4UFWPV9WzwH3ADQuWeRuwraqeAJgXGi8HXgd8vD//2dOFhiTpzGgzOCaAJ+dNH+zPm+8y4NwkX0qyM8k7+vNfCRwGPpFkV5J7krysxVolSQNqMziyyLyF58XWAlcDbwY2Ab+e5LL+/FcBH6uqjcBR4GRjJDcnmU4yffjw4WUrXpK0uDaD4yBw4bzpC4BDiyzzYFUdrapngEeAK/vzD1bVV/vLfZZekPyAqrq7qiaranL9+vXL+gUkST+ozZYjO4BLk1wCzAA30RvTmO9+4LeSrAXOAq4FfrOq/iLJk0k2VNV+4A3A15HUmL2VummYv9fWgqOqnktyKzAFrAHuraq9SW7pv35XVe1L8iDwKPA8cE9VPdZ/i/cAn+pfUfU48K62apW6yt5K3TTs36tPAJQ67DUf/ENmFmmHMTE+xv++/WeHUJGWw3L+XkftclxJQ2ZvpW4a9u/V4JA6zN5K3TTs36vBIXWYvZW6adi/Vx/kJHXY3ECpV1V1y7B/rw6OS9Iq5uC4JKl1BockqRGDQ5LUiMEhSWrEq6rUSfZn0mJGZb8YlTpeLINDnTPsPj4aTaOyX4xKHUvhqSp1ztap/S/8Rzln9thxtk7tH1JFGgWjsl+MSh1LYXCoc4bdx0ejaVT2i1GpYykMDnXOsPv4aDSNyn4xKnUshcGhzhl2Hx+NplHZL0aljqVwcFydM+w+PhpNo7JfjEodS2GvKklaxexVJUlqncEhSWrE4JAkNeLguEbSSm/JIHWZwaGR04WWDFKXeapKI6cLLRmkLjM4NHK60JJB6jKDQyOnCy0ZpC4zODRyutCSQeoyB8c1crrQkkHqMoNDI2nzxgmDQhpRnqqSJDVicEiSGjE4JEmNGBySpEYMDklSI60GR5Lrk+xPciDJ7SdZ5roku5PsTfLwgtfWJNmV5PfbrFOSNLjWLsdNsga4E3gjcBDYkeSBqvr6vGXGgY8C11fVE0leseBt3gvsA17eVp2SpGbaPOK4BjhQVY9X1bPAfcANC5Z5G7Ctqp4AqKqn515IcgHwZuCeFmuUJDXUZnBMAE/Omz7YnzffZcC5Sb6UZGeSd8x77cPAvwGeP9WHJLk5yXSS6cOHDy9D2ZKkU2nzzvEsMq8W+fyrgTcAY8BXkvwJvUB5uqp2JrnuVB9SVXcDdwNMTk4ufH9J0jJrMzgOAhfOm74AOLTIMs9U1VHgaJJHgCuBVwE/n+TngJcCL0/y36rq7S3WK0kaQJunqnYAlya5JMlZwE3AAwuWuR94bZK1Sc4GrgX2VdUdVXVBVV3cX+8PDQ1JGg2tHXFU1XNJbgWmgDXAvVW1N8kt/dfvqqp9SR4EHqU3lnFPVT3WVk2SpKVLVXeGBSYnJ2t6enrYZUjSipFkZ1VNNlnHO8clSY0YHJKkRgYa40jyOeBe4H9W1Snvq5C275rx6X1Shw16xPExend5/3mSDyb5iRZr0gq2fdcMd2zbw8yRWQqYOTLLHdv2sH3XzLBLk7RMBgqOqvqDqvqn9O6v+AbwUJI/TvKuJOvaLFAry9ap/cweO37CvNljx9k6tX9IFUlabgOPcST5a8A/B34R2AV8hF6QPNRKZVqRDh2ZbTRf0soz6BjHNuAngP8K/MOqeqr/0u8l8fpXveD88TFmFgmJ88fHhlCNpDYMesTxW1V1eVVtmRcaADS9/lfddtumDYytW3PCvLF1a7ht04YhVSRpuQ0aHD/Zf3YGAEnOTfIv2ylJK9nmjRNsufEKJsbHCDAxPsaWG6/wqiqpQwa6czzJ7qq6asG8XVW1sa3CXgzvHJekZtq8c/wlSV5ok95/ut9ZTT5IktQNgzY5nAI+k+Ques/UuAV4sLWqJEkja9Dg+LfALwG/TO8BTV/AR7pK0qo0UHD024x8rP9Hi7DNhqTVYtD7OC4FtgCX03siHwBV9cqW6lpR5tpszN0xPddmAzA8JHXOoIPjn6B3tPEc8Hrgd+jdDChssyFpdRk0OMaq6ov0Lt/9ZlV9APjZ9spaWWyzIWk1GXRw/PtJXkKvO+6twAzwivbKWllssyFpNRn0iON9wNnArwBXA28H3tlSTSuObTYkrSanPeLo3+z3T6rqNuB7wLtar2qFmRsA96oqSavBaYOjqo4nuTpJapD+JKvU5o0TBoWkVWHQMY5dwP1J/jtwdG5mVW1rpSpJ0sgaNDh+FPhLTrySqgCDQ5JWmUHvHHdcQ5IEDH7n+CfoHWGcoKr+xbJXJEkaaYOeqvr9eT+/FHgLcGj5y9Gw2XNL0ukMeqrqc/Onk3wa+INWKtLQ2HNL0iAGvQFwoUuBi5azEA2fPbckDWLQMY7vcuIYx1/Qe0aHOsSeW5IGMeipqnPaLkTDZ88tSYMY6FRVkrck+ZF50+NJNrdWlYbCnluSBjHoGMf7q+rbcxNVdQR4fysVaWg2b5xgy41XMDE+RoCJ8TG23HiFA+OSTjDo5biLBcwgDRKvBz4CrAHuqaoPLrLMdcCHgXXAM1X1d5NcSO9hUT8GPA/cXVUfGbBWLYE9tySdzqDBMZ3kQ8Cd9AbJ3wPsPNUK/a66dwJvBA4CO5I8UFVfn7fMOPBR4PqqeiLJ3DM+ngN+taq+luQcYGeSh+avK0kajkFPVb0HeBb4PeAzwCzw7tOscw1woKoer6pngfuAGxYs8zZgW1U9AVBVT/f/fqqqvtb/+bvAPsD/DZakETDoVVVHgdsbvvcE8OS86YPAtQuWuQxYl+RLwDnAR6rqd+YvkORiYCPw1cU+JMnNwM0AF13krSWS1LZBr6p6qH9aaW763CRTp1ttkXkL+12tpfdEwTcDm4BfT3LZvM/5YeBzwPuq6juLfUhV3V1Vk1U1uX79+tN/GUnSkgw6xnFe/0oqAKrqr+aNR5zMQeDCedMX8IP9rQ7SGxA/ChxN8ghwJfBnSdbRC41P+dwPSRodg45xPJ/khfNA/dNHp3sa4A7g0iSXJDkLuAl4YMEy9wOvTbI2ydn0TmXtSxLg48C+qvrQgDVKks6AQY84/h3wv5I83J9+Hf1xhZOpqueS3ApM0bsc996q2pvklv7rd1XVviQPAo/Su+z2nqp6LMnPAP8M2JNkd/8tf62qPt/ky0mSll8GfYx4/9TUzcBueq3Vn66qR9orrbnJycmanp4edhmStGIk2VlVk03WGbTJ4S8C76U3TrEbeDXwFU58lKwkaRUYdIzjvcBPAd+sqtfTuzz2cGtVSZJG1qDB8f2q+j5Akh+qqj8F7HwnSavQoIPjB/v3cWwHHkryV/joWElalQa9c/wt/R8/kOSPgB8BHmytKknSyBr0iOMFVfXw6ZeSJHXVi33muCRplTI4JEmNGBySpEYMDklSIwaHJKkRg0OS1IjBIUlqxOCQJDVicEiSGjE4JEmNGBySpEYMDklSIwaHJKmRxt1xu2j7rhm2Tu3n0JFZzh8f47ZNG9i8cWLYZUnSSFr1wbF91wx3bNvD7LHjAMwcmeWObXsADA9JWsSqP1W1dWr/C6ExZ/bYcbZO7R9SRZI02lZ9cBw6MttoviStdqs+OM4fH2s0X5JWu1UfHLdt2sDYujUnzBtbt4bbNm0YUkWSNNpW/eD43AC4V1VJ0mBWfXBALzwMCkkazKo/VSVJasbgkCQ1YnBIkhpxjKNjbJ8iqW0GR4fYPkXSmeCpqg6xfYqkM6HV4EhyfZL9SQ4kuf0ky1yXZHeSvUkebrKuTmT7FElnQmvBkWQNcCfwJuBy4K1JLl+wzDjwUeDnq+pvAb8w6Lr6QbZPkXQmtHnEcQ1woKoer6pngfuAGxYs8zZgW1U9AVBVTzdYVwvYPkXSmdBmcEwAT86bPtifN99lwLlJvpRkZ5J3NFgXgCQ3J5lOMn348OFlKn1l2rxxgi03XsHE+BgBJsbH2HLjFQ6MS1pWbV5VlUXm1SKffzXwBmAM+EqSPxlw3d7MqruBuwEmJycXXWY1sX2KpLa1GRwHgQvnTV8AHFpkmWeq6ihwNMkjwJUDritJGoI2T1XtAC5NckmSs4CbgAcWLHM/8Noka5OcDVwL7BtwXUnSELR2xFFVzyW5FZgC1gD3VtXeJLf0X7+rqvYleRB4FHgeuKeqHgNYbN22apUkDS5V3RkWmJycrOnp6WGXIUkrRpKdVTXZZB3vHJckNWJwSJIaMTgkSY0YHJKkRgwOSVIjBockqRGDQ5LUiMEhSWrE4JAkNWJwSJIaMTgkSY0YHJKkRgwOSVIjBockqRGDQ5LUiMEhSWrE4JAkNWJwSJIaMTgkSY0YHJKkRgwOSVIjBockqRGDQ5LUiMEhSWrE4JAkNWJwSJIaMTgkSY0YHJKkRgwOSVIjBockqRGDQ5LUiMEhSWqk1eBIcn2S/UkOJLl9kdevS/LtJLv7f/79vNf+VZK9SR5L8ukkL22zVknSYFoLjiRrgDuBNwGXA29Ncvkii365qq7q//kP/XUngF8BJqvqbwNrgJvaqlWSNLg2jziuAQ5U1eNV9SxwH3BDg/XXAmNJ1gJnA4daqFGS1NDaFt97Anhy3vRB4NpFlvvpJP+HXjD866raW1UzSX4DeAKYBb5QVV9Y7EOS3AzcDHDRRRctZ/1n3PZdM2yd2s+hI7OcPz7GbZs2sHnjxLDLkqQTtHnEkUXm1YLprwF/s6quBP4zsB0gybn0jk4uAc4HXpbk7Yt9SFXdXVWTVTW5fv365ar9jNu+a4Y7tu1h5sgsBcwcmeWObXvYvmtm2KVJ0gnaDI6DwIXzpi9gwemmqvpOVX2v//PngXVJzgP+HvB/q+pwVR0DtgF/p8Vah27r1H5mjx0/Yd7sseNsndo/pIokaXFtBscO4NIklyQ5i97g9gPzF0jyY0nS//mafj1/Se8U1auTnN1//Q3AvhZrHbpDR2YbzZekYWltjKOqnktyKzBF76qoe6tqb5Jb+q/fBfxj4JeTPEdvLOOmqirgq0k+S+9U1nPALuDutmodBeePjzGzSEicPz42hGok6eTS+3e6GyYnJ2t6enrYZbwoc2Mc809Xja1bw5Ybr3CAXFJrkuysqskm67R5VZUamAsHr6qSNOoMjhGyeeOEQSFp5NmrSpLUiMEhSWrE4JAkNWJwSJIaMTgkSY106j6OJN8F7NGxPM4Dnhl2ER3i9lxebs/ls6GqzmmyQtcux93f9EYWLS7JtNty+bg9l5fbc/kkaXzXtKeqJEmNGBySpEa6FhydboR4hrktl5fbc3m5PZdP423ZqcFxSVL7unbEIUlqmcEhSWqkE8GR5Pok+5McSHL7sOtZ6ZJ8I8meJLtfzKV6q12Se5M8neSxefN+NMlDSf68//e5w6xxpTjJtvxAkpn+/rk7yc8Ns8aVJMmFSf4oyb4ke5O8tz+/0f654oMjyRrgTuBNwOXAW5NcPtyqOuH1VXWV18q/KL8NXL9g3u3AF6vqUuCL/Wmd3m/zg9sS4Df7++dVVfX5M1zTSvYc8KtV9ZPAq4F39/+9bLR/rvjgAK4BDlTV41X1LHAfcMOQa9IqVlWPAP9vwewbgE/2f/4ksPlM1rRSnWRb6kWqqqeq6mv9n78L7AMmaLh/diE4JoAn500f7M/Ti1fAF5LsTHLzsIvpiL9eVU9B7z9e4BVDrmeluzXJo/1TWZ72exGSXAxsBL5Kw/2zC8GRReZ5jfHSvKaqXkXv9N+7k7xu2AVJ83wM+HHgKuAp4D8NtZoVKMkPA58D3ldV32m6fheC4yBw4bzpC4BDQ6qlE6rqUP/vp4H/Qe90oJbmW0n+BkD/76eHXM+KVVXfqqrjVfU88F9w/2wkyTp6ofGpqtrWn91o/+xCcOwALk1ySZKzgJuAB4Zc04qV5GVJzpn7Gfj7wGOnXksDeAB4Z//ndwL3D7GWFW3uH7i+t+D+ObAkAT4O7KuqD817qdH+2Yk7x/uX430YWAPcW1X/cbgVrVxJXknvKAN63ZN/1+3ZTJJPA9fRa/39LeD9wHbgM8BFwBPAL1SVg76ncZJteR2901QFfAP4pbnz8zq1JD8DfBnYAzzfn/1r9MY5Bt4/OxEckqQzpwunqiRJZ5DBIUlqxOCQJDVicEiSGjE4JEmNGBxSi5JcPL+zq9QFBockqRGDQzpDkrwyya4kPzXsWqSlMDikMyDJBnr9gd5VVTuGXY+0FGuHXYC0Cqyn1/vnH1XV3mEXIy2VRxxS+75N75kxrxl2IdJy8IhDat+z9J6oNpXke1X1u0OuR1oSg0M6A6rqaJJ/ADyU5GhV2VZdK5bdcSVJjTjGIUlqxOCQJDVicEiSGjE4JEmNGBySpEYMDklSIwaHJKmR/w/HrovC2QQVcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "k_range = range(1, 20)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    scores.append(knn.score(X_test, y_test))\n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.xticks([0,5,10,15,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #7 Which model performs best on our out-of-sample data from validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The accuracy for our model is 67.2897%\n",
      "    The True Positive Rate is 48.780%,    The False Positive Rate is 21.212%,\n",
      "    The True Negative Rate is 78.788%,    The False Negative Rate is 51.220%\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "\n",
      "    The positive is  'survived'\n",
      "    Confusion Matrix\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_dead</th>\n",
       "      <th>pred_survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_dead</th>\n",
       "      <td>True Negative: 104</td>\n",
       "      <td>False positive: 28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_survived</th>\n",
       "      <td>False Negative: 42</td>\n",
       "      <td>True Positive: 40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          pred_dead       pred_survived\n",
       "actual_dead      True Negative: 104  False positive: 28\n",
       "actual_survived  False Negative: 42   True Positive: 40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "    Classification Report:\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dead</th>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.748201</td>\n",
       "      <td>132.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.672897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.650282</td>\n",
       "      <td>0.637842</td>\n",
       "      <td>0.640767</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.664779</td>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.665869</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "dead           0.712329  0.787879  0.748201  132.000000\n",
       "survived       0.588235  0.487805  0.533333   82.000000\n",
       "accuracy       0.672897  0.672897  0.672897    0.672897\n",
       "macro avg      0.650282  0.637842  0.640767  214.000000\n",
       "weighted avg   0.664779  0.672897  0.665869  214.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_fun_cindy.model_performs(X_validate, y_validate, knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
